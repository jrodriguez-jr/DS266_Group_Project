{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7f697fec28a"
      },
      "source": [
        "\n",
        "\n",
        "<p>-Prepares FastText Twitter Embeddings for Transformer model</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFwSaNB8jF7s"
      },
      "source": [
        "<style>\n",
        "td {\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        "th {\n",
        "  text-align: center;\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swymtxpl7W7w"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following codes ensures the words embeddings have the same result after being randomly initialized"
      ],
      "metadata": {
        "id": "fUVUPmpZwfcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uj-XUHm-wWdL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acgF9rXfaiRP",
        "outputId": "7290fc8f-c29b-42fa-ed06-c2bcf7819606"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the Directory Containing the CSV File\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/DS266 Project')"
      ],
      "metadata": {
        "id": "FNO7MPwxarDt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfV1batAwq9j"
      },
      "source": [
        "Begin by installing [TensorFlow Datasets](https://tensorflow.org/datasets) for loading the dataset and [TensorFlow Text](https://www.tensorflow.org/text) for text preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XFG0NDRu5mYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f3fb4f-50c5-49e4-861d-f37de540efb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package libcudnn8 is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "\u001b[1;31mE: \u001b[0mVersion '8.1.0.77-1+cuda11.2' for 'libcudnn8' was not found\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 -q\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text -q\n",
        "!pip install -q protobuf~=3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install compatible TensorFlow version\n",
        "!pip uninstall tensorflow -y -q\n",
        "!pip install tensorflow==2.15 -q\n",
        "\n",
        "# Reinstall tf-keras to resolve any potential dependency conflicts\n",
        "!pip uninstall tf-keras -y -q\n",
        "!pip install tf-keras -q"
      ],
      "metadata": {
        "id": "sfB0JATpjTEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15916511-b46d-479a-805e-e5459d598495"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15 -q #it needs it again for some reason"
      ],
      "metadata": {
        "id": "Q-VgaWpbm21T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82d6aa2-ee0b-4de6-e150-9c46169635c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow-text -y -q\n",
        "!pip install tensorflow-text==2.15 -q"
      ],
      "metadata": {
        "id": "KHCOdSJLuYLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88274f21-6b40-49f1-dd20-ae487d7ade31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim #needed to import fast text twitter embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXzuBvk7mGhm",
        "outputId": "d041a219-dc2c-4441-9637-b4713dbea138"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GYpLBSjxJmG"
      },
      "source": [
        "Import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text as text\n",
        "from tensorflow import keras\n",
        "\n",
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "J9X0hde60st6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf_WUi2HLhzf"
      },
      "source": [
        "## Data handling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cCvXbPkccV1"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTEVgBxklzdq"
      },
      "source": [
        "read the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV File\n",
        "import pandas as pd\n",
        "load_data = 'train_enhanced_cleaned.csv'\n",
        "tweets_df = pd.read_csv(load_data).drop_duplicates()\n",
        "tweets_df.tail(3)"
      ],
      "metadata": {
        "id": "lw7axoYfayav",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "8c32ec1c-4ebb-4272-9af2-374cb5efa137"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  target\n",
              "7610  location   m1 94 0104 utc5km s of volcano hawa...       1\n",
              "7611  location   police investigating after an ebike...       1\n",
              "7612  location   the latest more homes razed by nort...       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03a29a68-ec5a-4a33-a43e-f67f1984d311\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>location   m1 94 0104 utc5km s of volcano hawa...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>location   police investigating after an ebike...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>location   the latest more homes razed by nort...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03a29a68-ec5a-4a33-a43e-f67f1984d311')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03a29a68-ec5a-4a33-a43e-f67f1984d311 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03a29a68-ec5a-4a33-a43e-f67f1984d311');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f996377b-fbfc-48a2-9db9-d23c4a65aebb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f996377b-fbfc-48a2-9db9-d23c4a65aebb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f996377b-fbfc-48a2-9db9-d23c4a65aebb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"tweets_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"location   m1 94 0104 utc5km s of volcano hawaii  weblink   neutral  news   social concern non irony joy \",\n          \"location   police investigating after an ebike collided with a car in little portugal  ebike rider suffered serious nonlife threatening injuries  negative  news   social concern non irony fear \",\n          \"location   the latest more homes razed by northern california wildfire  abc news weblink   neutral  news   social concern irony fear \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load FastText and Setup"
      ],
      "metadata": {
        "id": "qgth7XuujkCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The “fasttext_english_twitter_100d.vec” file is a set of word vectors trained using the FastText library.\n",
        "\n",
        "FastText is a library developed by Facebook’s AI Research (FAIR) lab. It’s dedicated to text classification and learning word representations. FastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.\n",
        "\n",
        "Here’s a general process of how FastText trains word vectors:\n",
        "\n",
        "Corpus Selection: FastText trains word vectors on large text corpora. For example, FastText provides pre-trained word vectors trained, in this case, a collection of English tweets.\n",
        "Model Training: These models are trained using the Continuous Bag of Words (CBOW) model with position-weights, in a specified dimension (like 300), with character n-grams of length 5, a window of size 5, and 10 negatives2.\n",
        "Vector Generation: Each line of the output file contains a word followed by its vectors, like in the default FastText text format. Each value is space-separated. Words are ordered by descending frequency.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kZQ1FZeGluTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vectors directly from the file\n",
        "vector_model = KeyedVectors.load_word2vec_format('fasttext_english_twitter_100d.vec')"
      ],
      "metadata": {
        "id": "UQxLXk7gjnOP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading Fast Text Tweet Embeddings we did a quick sanity check to see if some of the words we anticipate to commonly show up in our tweets text are captured."
      ],
      "metadata": {
        "id": "Ot3FWEyLmbVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_word(item_value, model):\n",
        "  word = None\n",
        "  #print(model[item_value])\n",
        "  #print(item_value)\n",
        "  try:\n",
        "    if model[item_value].any():\n",
        "      word = \"\\\"\"+item_value +\"\\\" exists in embeddings dictionary\"\n",
        "      vector = model[item_value]\n",
        "  except:\n",
        "    word = \"\\\"\"+item_value +\"\\\" does not exist\"\n",
        "    vector = np.zeros((100,))\n",
        "  return word, vector\n",
        "\n",
        "check = ['fire', 'hurricane', 'earthquake', 'sunami', 'disaster']\n",
        "for i in range(len(check)-0):\n",
        "  word, vector = check_word(check[i], vector_model)\n",
        "  print(word)\n",
        "  print(vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTzu5KUxmcWK",
        "outputId": "fd204cb1-d6ab-4f75-b60b-12ce121d8e00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"fire\" exists in embeddings dictionary\n",
            "(100,)\n",
            "\"hurricane\" exists in embeddings dictionary\n",
            "(100,)\n",
            "\"earthquake\" exists in embeddings dictionary\n",
            "(100,)\n",
            "\"sunami\" does not exist\n",
            "(100,)\n",
            "\"disaster\" exists in embeddings dictionary\n",
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJxTd6aVnZyh"
      },
      "source": [
        "### Set up the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mopr6oKUlzds"
      },
      "source": [
        "Now that you have loaded the dataset, you need to tokenize the text, so that each element is represented as a [token](https://developers.google.com/machine-learning/glossary#token) or token ID (a numeric representation).\n",
        "\n",
        "Tokenization is the process of breaking up text, into \"tokens\". Depending on the tokenizer, these tokens can represent sentence-pieces, words, subwords, or characters. To learn more about tokenization, visit [this guide](https://www.tensorflow.org/text/guide/tokenizers)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = tweets_df['text']\n",
        "labels = tweets_df['target']\n",
        "sentences.shape"
      ],
      "metadata": {
        "id": "WiZB9l9ubV0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf05c95-e873-497b-f20f-f957c726316b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7066,)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on your text data\n",
        "tokenizer.fit_on_texts(sentences)\n"
      ],
      "metadata": {
        "id": "FlLcttf77sHP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####MAX_TOKENS"
      ],
      "metadata": {
        "id": "RAhQ5dE8yyYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS =30"
      ],
      "metadata": {
        "id": "t4ZcNHiEyzbd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences) #sequences needed for batching set up\n",
        "\n",
        "# Pad sequences\n",
        "sequences = pad_sequences(sequences, maxlen=MAX_TOKENS, padding='post') #sequences needed for batching set up\n",
        "\n"
      ],
      "metadata": {
        "id": "g2IH6BJs7hGZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####convert back to a word"
      ],
      "metadata": {
        "id": "ySIuwa2iovLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "tokenized_sentences = []\n",
        "for sequence in sequences:\n",
        "    words = [tokenizer.index_word[token] for token in sequence if token in tokenizer.index_word]\n",
        "    sentence = ' '.join(words)\n",
        "    tokenized_sentences.append(sentence)"
      ],
      "metadata": {
        "id": "poi-mXKlouSf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenize and convert to FastText Twitter Embedding"
      ],
      "metadata": {
        "id": "ECjeJWSaoX9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Vector Dim\n",
        "vector_dim must be equal to the hyperparameter of d_model"
      ],
      "metadata": {
        "id": "_5MjxDWLxW7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_dim = 100 #equal to hyperparameter d_model"
      ],
      "metadata": {
        "id": "eyT1-tCoxe6X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocab size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for the padding token\n",
        "\n",
        "# Initialize the embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((vocab_size, vector_dim))\n",
        "\n",
        "# Fill the embedding matrix with vectors from the FastText model\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if word in vector_model:\n",
        "        embedding_vector = vector_model[word] #FastText Twitter Embeddings\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "\n",
        "# Convert the embedding matrix to a DataFrame\n",
        "embedding_df = pd.DataFrame(embedding_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "r6-xTbMkoUMF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TftI7YGcjqG",
        "outputId": "9bf1d369-5ce9-4b7b-e495-37750011e68b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17984, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Write to file"
      ],
      "metadata": {
        "id": "oXnnwOq0IkiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "embedding_df.to_csv('fast_text_twitter_embeddings.csv', index=False)"
      ],
      "metadata": {
        "id": "OBZaeo0jIfNb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking the Vectorization"
      ],
      "metadata": {
        "id": "H9hx2LWmzEMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Checks"
      ],
      "metadata": {
        "id": "2jIrTnxunFhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_embeddings_df = pd.read_csv('fast_text_twitter_embeddings.csv')"
      ],
      "metadata": {
        "id": "MAOGpmBknMFy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize a word and get its token ID\n",
        "word_to_test = 'house'\n",
        "token_id = tokenizer.word_index[word_to_test]\n",
        "\n",
        "# Load the corresponding vector from the CSV\n",
        "vector_from_csv = loaded_embeddings_df.iloc[token_id - 1].values  # -1 because indexing starts at 0\n",
        "\n",
        "# Print the vector to verify\n",
        "print(f\"Vector for the word '{word_to_test}' from CSV:\", vector_from_csv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYr1Qkn2pk_0",
        "outputId": "441430e8-f548-4f8d-fe88-fee338f52a38"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for the word 'house' from CSV: [ 0.12065     0.14212     0.17784999  0.18043999 -0.21229     0.30074999\n",
            " -0.076699   -0.23617999 -0.33881     0.26030999 -0.41927001  0.81598997\n",
            "  0.38793999  0.34344     0.40472001 -0.22395    -0.91606998 -0.061257\n",
            " -0.56077999  0.11166    -0.047045   -0.22796001 -0.21043     0.26730001\n",
            "  0.058979    0.37796    -0.067553   -0.021946    0.069971    0.035356\n",
            " -0.43852001 -0.055163   -0.26499999 -0.20479     0.44907999 -0.042062\n",
            " -0.35530001  0.51872998  0.30981001  0.0067503  -0.46794999  0.50919998\n",
            " -0.38045001  0.010086    0.013727   -0.056027    0.07452     0.36842\n",
            "  0.028126   -0.14568    -0.47525001  0.15092     0.033576    0.33785\n",
            "  0.32330999 -0.26062    -0.38325    -0.14787     0.18810999  0.87696999\n",
            "  0.30160999 -0.081211    0.18231    -0.062108    0.92413998  0.31123999\n",
            " -0.71069998  0.052805   -0.39974999 -0.32308     0.085745    0.43895\n",
            "  0.34470001  0.3231      0.17323001  0.082239    0.02889    -0.18970001\n",
            "  0.13151     0.09467    -0.096707   -0.24155    -0.15761    -0.16864\n",
            "  0.38324001  0.060199    0.23947001 -0.17021    -0.12348     0.057802\n",
            " -0.1681     -0.30607     0.14526001 -0.046102    0.15376     0.093508\n",
            " -0.27333999 -0.24724001 -0.30897    -0.24879999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format('fasttext_english_twitter_100d.vec')\n",
        "\n"
      ],
      "metadata": {
        "id": "j-xUphCp2V6R"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Ensure the word is in the tokenizer's vocabulary"
      ],
      "metadata": {
        "id": "tqa0KAiqzOP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure the word is in the tokenizer's vocabulary\n",
        "word_to_test = 'house'\n",
        "if word_to_test in tokenizer.word_index:\n",
        "    token_id = tokenizer.word_index[word_to_test]\n",
        "else:\n",
        "    raise ValueError(f\"'{word_to_test}' not found in tokenizer's vocabulary.\")\n",
        "\n",
        "\n",
        "vector_from_csv = loaded_embeddings_df.iloc[token_id].values\n",
        "\n",
        "# Get the vector for the word from the FastText model\n",
        "if word_to_test in vector_model:\n",
        "    vector_from_model = vector_model[word_to_test]\n",
        "else:\n",
        "    raise ValueError(f\"Vector for '{word_to_test}' not found in FastText model.\")\n",
        "\n",
        "# Compare the two vectors\n",
        "are_vectors_equal = np.allclose(vector_from_csv, vector_from_model, atol=1e-9)\n",
        "\n",
        "# Print the result of the comparison\n",
        "print(f\"Does the vector for '{word_to_test}' from CSV match the FastText model? {are_vectors_equal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2CVU8p13YiV",
        "outputId": "d4efa728-28b7-4fdc-8605-515504ee1537"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does the vector for 'house' from CSV match the FastText model? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visual verification"
      ],
      "metadata": {
        "id": "tU_YQJJU50Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(vector_from_model), vector_from_model.shape, vector_from_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR81FFuK3rxv",
        "outputId": "26e703de-31ee-4736-e05f-c4cd7d4e860b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray,\n",
              " (100,),\n",
              " array([ 4.5484e-01,  1.1877e-01, -6.7182e-01,  4.0004e-02,  2.9812e-01,\n",
              "         1.3266e-02,  2.2786e-01, -2.8099e-01, -4.4837e-02,  2.1004e-01,\n",
              "         7.1316e-02,  2.0716e-01,  6.1164e-01,  2.0849e-02,  7.6408e-02,\n",
              "         1.8154e-01, -5.7663e-01,  1.4443e-01, -5.2093e-02,  1.3182e-01,\n",
              "         1.3530e-01,  1.5884e-01,  4.9659e-01,  8.6108e-02,  5.4261e-01,\n",
              "         2.9661e-01, -1.9361e-01, -1.5841e-01,  9.3623e-02, -3.1574e-01,\n",
              "         1.6179e-01,  4.0957e-01, -1.6413e-02,  9.4639e-02,  1.5520e-01,\n",
              "        -2.4322e-02, -2.4437e-01,  3.2023e-04,  6.7701e-02,  4.5904e-01,\n",
              "         4.2513e-02,  2.0157e-01,  2.9248e-01, -2.3573e-01,  1.3197e-01,\n",
              "         1.9594e-02, -4.6181e-01,  1.2979e-01, -5.8775e-01, -1.6625e-01,\n",
              "         9.8981e-01,  1.5905e-01, -6.1933e-02, -1.0294e-01, -3.5458e-03,\n",
              "        -2.7351e-01, -2.3616e-01,  7.1419e-02,  4.0173e-01,  5.4632e-01,\n",
              "         1.4278e-01, -1.8228e-01, -2.2210e-01, -8.3939e-01,  6.5804e-01,\n",
              "         7.0661e-01, -7.1321e-01, -2.0366e-01, -2.9752e-01,  7.3056e-02,\n",
              "         5.6124e-03,  3.5355e-01, -2.1702e-01, -9.9723e-02, -7.9362e-01,\n",
              "         4.2521e-01,  2.7265e-01,  1.1304e-02,  2.0000e-01,  3.7437e-01,\n",
              "         1.1671e-01,  2.7043e-01,  4.7604e-01, -6.4862e-02, -3.6681e-01,\n",
              "         5.5360e-01,  2.4873e-01,  2.3061e-01,  2.0558e-02,  7.7409e-02,\n",
              "        -3.2574e-01, -4.4151e-01,  4.0960e-02, -3.5805e-01,  1.9104e-01,\n",
              "         3.8063e-01, -2.1004e-01,  1.0808e-01, -4.6669e-02, -2.2271e-01],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(vector_from_csv), vector_from_csv.shape, vector_from_csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erz00jwZ3w8P",
        "outputId": "e6c91fcb-4f9a-464a-a604-70fc3e434c2f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray,\n",
              " (100,),\n",
              " array([ 4.54840004e-01,  1.18770003e-01, -6.71819985e-01,  4.00040001e-02,\n",
              "         2.98119992e-01,  1.32660000e-02,  2.27860004e-01, -2.80990005e-01,\n",
              "        -4.48370017e-02,  2.10040003e-01,  7.13159963e-02,  2.07159996e-01,\n",
              "         6.11639977e-01,  2.08490007e-02,  7.64079988e-02,  1.81539997e-01,\n",
              "        -5.76629996e-01,  1.44429997e-01, -5.20929992e-02,  1.31819993e-01,\n",
              "         1.35299996e-01,  1.58840001e-01,  4.96589988e-01,  8.61079991e-02,\n",
              "         5.42609990e-01,  2.96609998e-01, -1.93609998e-01, -1.58409998e-01,\n",
              "         9.36229974e-02, -3.15739989e-01,  1.61789998e-01,  4.09570009e-01,\n",
              "        -1.64129995e-02,  9.46390033e-02,  1.55200005e-01, -2.43219994e-02,\n",
              "        -2.44369999e-01,  3.20229999e-04,  6.77009970e-02,  4.59039986e-01,\n",
              "         4.25130017e-02,  2.01570004e-01,  2.92479992e-01, -2.35730007e-01,\n",
              "         1.31970003e-01,  1.95940007e-02, -4.61809993e-01,  1.29789993e-01,\n",
              "        -5.87750018e-01, -1.66250005e-01,  9.89809990e-01,  1.59050003e-01,\n",
              "        -6.19329996e-02, -1.02940001e-01, -3.54579999e-03, -2.73510009e-01,\n",
              "        -2.36159995e-01,  7.14190006e-02,  4.01730001e-01,  5.46320021e-01,\n",
              "         1.42780006e-01, -1.82280004e-01, -2.22100005e-01, -8.39389980e-01,\n",
              "         6.58039987e-01,  7.06610024e-01, -7.13209987e-01, -2.03659996e-01,\n",
              "        -2.97520012e-01,  7.30559975e-02,  5.61239989e-03,  3.53549987e-01,\n",
              "        -2.17020005e-01, -9.97229964e-02, -7.93619990e-01,  4.25209999e-01,\n",
              "         2.72650003e-01,  1.13040004e-02,  2.00000003e-01,  3.74370009e-01,\n",
              "         1.16710000e-01,  2.70429999e-01,  4.76040006e-01, -6.48619980e-02,\n",
              "        -3.66809994e-01,  5.53600013e-01,  2.48730004e-01,  2.30609998e-01,\n",
              "         2.05579996e-02,  7.74089992e-02, -3.25740010e-01, -4.41509992e-01,\n",
              "         4.09599990e-02, -3.58049989e-01,  1.91039994e-01,  3.80629987e-01,\n",
              "        -2.10040003e-01,  1.08080000e-01, -4.66689989e-02, -2.22709998e-01]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional verification"
      ],
      "metadata": {
        "id": "Iheue7K052E_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Load a Vector and Return a Word"
      ],
      "metadata": {
        "id": "ALXmpjKDKZcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the function to find the closest word for a given vector\n",
        "def find_closest_word(vector, nn_model, tokenizer):\n",
        "    # Reshape the vector to match the input shape for the model (1, number of features)\n",
        "    vector = vector.reshape(1, -1)\n",
        "\n",
        "    # Find the nearest neighbor (closest vector in the space)\n",
        "    _, indices = nn_model.kneighbors(vector)\n",
        "\n",
        "    # Get the index of the closest vector\n",
        "    closest_index = indices[0][0]\n",
        "\n",
        "    # Find the corresponding word from the tokenizer\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == closest_index:\n",
        "            return word\n",
        "    return \"Word not found\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k6uwt6v631wq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'loaded_embeddings_df' contains all the vectors from the CSV\n",
        "# and 'tokenizer' is a dictionary mapping words to their indices\n",
        "\n",
        "# Convert the embeddings dataframe to a numpy array\n",
        "embeddings = loaded_embeddings_df.to_numpy()\n",
        "\n",
        "# Create and fit the NearestNeighbors model\n",
        "nn_model = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
        "nn_model.fit(embeddings)\n",
        "# Example usage:\n",
        "# Assuming 'vector_from_csv' is the vector you want to find the word for\n",
        "word = find_closest_word(vector_from_csv, nn_model, tokenizer)\n",
        "print(f\"The word for the given vector is: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fMEcGaO6HSI",
        "outputId": "cac6aaae-de59-4159-edfb-9226670f2d1d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word for the given vector is: house\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#End of File\n",
        "At this point the embeddings for loading to the transformer have been created"
      ],
      "metadata": {
        "id": "ZGwZfK_h6eLm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}