{"cells":[{"cell_type":"markdown","metadata":{"id":"Loz8c8-qnhPr"},"source":["# Video Question Answering Model"]},{"cell_type":"markdown","metadata":{"id":"AuWdF6cUqnYy"},"source":["## Video Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNUMlLUgrDca","outputId":"a53dbca7-4a92-4ea8-f5f1-7a457288714d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q tf-models-official==2.4.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-NgxhRTqheM"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","from official.nlp import bert\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Step 1: Video Feature Extraction\n","class VideoFeatureExtractor(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(VideoFeatureExtractor, self).__init__()\n","        # Load a pre-trained 3D CNN model from TensorFlow Hub\n","        self.model = hub.KerasLayer(\"https://tfhub.dev/deepmind/i3d-kinetics-400/1\")\n","\n","    def call(self, video_frames):\n","        # `video_frames` should have shape (batch_size, num_frames, H, W, 3)\n","        return self.model(video_frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_64pe_jqrPm","outputId":"fc612366-9763-4573-b264-3b561adb9def"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 400)\n"]}],"source":["# For the video feature extractor\n","# Assume we use clips of 16 frames, each frame of size 224x224, and 3 color channels (RGB).\n","video_frames = tf.random.normal([1, 16, 224, 224, 3])\n","\n","# Testing VideoFeatureExtractor\n","video_feature_extractor = VideoFeatureExtractor()\n","video_extracted_features = video_feature_extractor(video_frames)\n","print(video_extracted_features.shape)  # Should match expected output shape"]},{"cell_type":"markdown","metadata":{"id":"gjfHeigwsW4k"},"source":["## Question Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JIO0S7fq3W0"},"outputs":[],"source":["# Step 2: Question Processing\n","class QuestionEncoder(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(QuestionEncoder, self).__init__()\n","        # Load BERT from tensorflow hub\n","        self.bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                                         trainable=False)\n","\n","    def call(self, input_ids, input_mask, segment_ids):\n","        pooled_output, sequence_output = self.bert_layer([input_ids, input_mask, segment_ids])\n","        return pooled_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hwrp9yM0sdmt"},"outputs":[],"source":["# Assuming we are using BERT-base, which has a hidden size of 768\n","# Let's simulate the pooled output from BERT\n","question_features = tf.random.normal([1, 768])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBFrL3JCse2u","outputId":"9335c8d2-3564-49eb-f07e-61ead8e32474"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 768)\n"]}],"source":["# Testing QuestionEncoder\n","question_encoder = QuestionEncoder()\n","# For simplicity let's assume these are the inputs after tokenization\n","dummy_input_ids = tf.random.uniform([1, 512], minval=0, maxval=2048, dtype=tf.int32)\n","dummy_input_mask = tf.random.uniform([1, 512], minval=0, maxval=2, dtype=tf.int32)\n","dummy_segment_ids = tf.random.uniform([1, 512], minval=0, maxval=2, dtype=tf.int32)\n","question_encoded_features = question_encoder(dummy_input_ids, dummy_input_mask, dummy_segment_ids)\n","print(question_encoded_features.shape)  # Should be (?, 768)"]},{"cell_type":"markdown","metadata":{"id":"51b0IcH7qrAg"},"source":["## Feature Fusion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Lu4wz7zsn_C"},"outputs":[],"source":["# Step 3: Feature Fusion\n","class FeatureFusion(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(FeatureFusion, self).__init__()\n","        self.fc = tf.keras.layers.Dense(512, activation='relu')\n","\n","    def call(self, video_features, question_features):\n","        combined = tf.concat([video_features, question_features], axis=-1)\n","        return self.fc(combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOiM5QAuss4F"},"outputs":[],"source":["# Feature fusion expects concatenation of video and question features\n","# Assuming video features are also encoded in a 768-dimensional vector\n","video_features = tf.random.normal([1, 768])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KxsvudVzsstz","outputId":"a389e3d7-2b70-48d3-ab1e-108552784e84"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 512)\n"]}],"source":["# Testing FeatureFusion\n","feature_fusion = FeatureFusion()\n","fused_features = feature_fusion(video_features, question_features)\n","print(fused_features.shape)  # Should be (?, 512) or the dimensionality chosen for fusion"]},{"cell_type":"markdown","metadata":{"id":"pTysCOu0tDb3"},"source":["## Answer Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2lnIkyPtMC9"},"outputs":[],"source":["# Step 4: Answer Generation\n","class AnswerGenerator(tf.keras.Model):\n","    def __init__(self, num_answers):\n","        super(AnswerGenerator, self).__init__()\n","        self.feature_fusion = FeatureFusion()\n","        self.output_layer = tf.keras.layers.Dense(num_answers, activation='softmax')\n","\n","    def call(self, video_features, question_features):\n","        fused_features = self.feature_fusion(video_features, question_features)\n","        return self.output_layer(fused_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdbRQyd3tM2C"},"outputs":[],"source":["# The fused feature vector would be the input to the answer generation step\n","fused_features = tf.random.normal([1, 512])  # Example size of the fused vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHZI8AUftV1C","outputId":"64a1d018-6275-4381-ce6f-eeb331ab007a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 1000)\n"]}],"source":["# Testing AnswerGenerator\n","answer_generator = AnswerGenerator(num_answers=1000)\n","predictions = answer_generator(fused_features, question_features)\n","print(predictions.shape)  # Should be (?, num_answers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udE69BphuDGR"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hhZkqrp1tI9i"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wBbYUvmn4Dq"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","from official.nlp import bert\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Step 1: Video Feature Extraction\n","class VideoFeatureExtractor(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(VideoFeatureExtractor, self).__init__()\n","        # Load a pre-trained 3D CNN model from TensorFlow Hub\n","        self.model = hub.KerasLayer(\"https://tfhub.dev/deepmind/i3d-kinetics-400/1\")\n","\n","    def call(self, video_frames):\n","        # `video_frames` should have shape (batch_size, num_frames, H, W, 3)\n","        return self.model(video_frames)\n","\n","# Step 2: Question Processing\n","class QuestionEncoder(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(QuestionEncoder, self).__init__()\n","        # Load BERT from tensorflow hub\n","        self.bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                                         trainable=False)\n","\n","    def call(self, input_ids, input_mask, segment_ids):\n","        pooled_output, sequence_output = self.bert_layer([input_ids, input_mask, segment_ids])\n","        return pooled_output\n","\n","# Step 3: Feature Fusion\n","class FeatureFusion(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(FeatureFusion, self).__init__()\n","        self.fc = tf.keras.layers.Dense(512, activation='relu')\n","\n","    def call(self, video_features, question_features):\n","        combined = tf.concat([video_features, question_features], axis=-1)\n","        return self.fc(combined)\n","\n","# Step 4: Answer Generation\n","class AnswerGenerator(tf.keras.Model):\n","    def __init__(self, num_answers):\n","        super(AnswerGenerator, self).__init__()\n","        self.feature_fusion = FeatureFusion()\n","        self.output_layer = tf.keras.layers.Dense(num_answers, activation='softmax')\n","\n","    def call(self, video_features, question_features):\n","        fused_features = self.feature_fusion(video_features, question_features)\n","        return self.output_layer(fused_features)\n","\n","# Questions need to be tokenized using BERT tokenizer\n","# Here, you would add code to tokenize the text input using a suitable tokenizer\n","tokenizer = Tokenizer()\n","input_ids = tokenizer.encode(text_input, add_special_tokens=True)\n","input_mask = [1] * len(input_ids)\n","segment_ids = [0] * len(input_ids)\n","\n","\n","# Dummy Input\n","video_frames = tf.random.normal([1, 16, 224, 224, 3])  # Example shape for batch_size x num_frames x H x W x Channels\n","text_input = \"What color is the cat?\"  # Placeholder for the actual question\n","# Tokenized text input\n","input_ids = ...\n","input_mask = ...\n","segment_ids = ...\n","\n","# Create Model\n","vqa_model = AnswerGenerator(num_answers=1000)\n","\n","# Forward Pass (Prediction Example)\n","video_features = VideoFeatureExtractor()(video_frames)\n","question_features = QuestionEncoder()(input_ids, input_mask, segment_ids)\n","predictions = vqa_model(video_features, question_features)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}